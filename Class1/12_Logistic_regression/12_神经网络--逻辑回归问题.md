逻辑回归（Logistic Regression）是一种用于解决二分类（0 or 1）问题的机器学习方法，用于估计某种事物的可能性。比如某用户购买某商品的可能性，某病人患有某种疾病的可能性，以及某广告被用户点击的可能性等。 注意，这里用的是“可能性”，而非数学上的“概率”，logisitc回归的结果并非数学定义中的概率值，不可以直接当做概率值来用。该结果往往用于和其他特征值加权求和，而非直接相乘。

本文将从一个简单的入学录取问题解释逻辑回归

# 如何判断学生是否应该被录取
假设你是负责招生的老师，给你的输入有两个：
- 学生的入学考试成绩（test）
- 学生的毕业考试成绩（grades）
已知：
学生A（Test：9，Grades：8）被录取，学生B（Test：3，Grades：4）不被录取，现有一个学生C（Test：7，Grades：6）是否应该被录取？

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/%E5%85%A5%E5%AD%A6%E9%97%AE%E9%A2%981.jpg)

## 历史数据
我们使用一个二维坐标画出学生A，学生B的成绩，横坐标表示Test，纵坐标表示Grades

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/6.%20Logistic%20Regression%20Quiz.mp4_000058.333.jpg)

再把历史录取与不录取的数据全部放到这个坐标系中,绿色点表示被录取的学生成绩，红色表示不被录取的学生成绩。基于这些历史数据，我们可以判断学生C是否应该被录取

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/6.%20Logistic%20Regression%20Quiz.mp4_000115.000.jpg)

## 解决录取问题
首先观察这个数据表，似乎有比较明显的分界线可以用直线表示：

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000011.304.jpg)

从图上可以看出直线上方大部分是绿点，下方大部分是红点，当然也有一些例外。我们直接以这条直线为依据，将学生C的成绩放入坐标系中，可以看出学生C的成绩在直线上方，应该被录取。现在问题来了，这条直线式我们按照直觉划的，我们怎么让计算机找到最优的直线呢？

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000041.145.jpg)

# 找到最优分界线
如下的分类问题中计算机是无法一眼画出直线的，所以我们会首先随机画一条直线做分割。直线以上的为绿色分类，直线以下的为红色分类。如同线性回归一样，我们需要统计误差，最常用的误差统计是统计错误的个数，当前这条直线划分后共有两个错误。

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000110.093.jpg)

## 梯度下降
如同线性回归，我们使用梯度下降算法来移动直线，目的是使误差最小化。我们沿一个方向转动直线，发现有一个错误点已经被正确归类了，误差减小。沿着这个方向再继续移动，误差进一步降低为0.

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000137.796.jpg)

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000154.286.jpg)

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000203.639.jpg)

在真正使用梯度下降算法时我们无需直接最小化错误数量，我们用对数损失函数（log loss function）来代表错误数量。

## 对数损失函数（log loss function
回到最初的图片，我们随机划了一条直线之后有一绿一红两个错误点，误差函数对这两个错误点赋予较高的惩罚值。并对四个正确点赋予较低惩罚值。然后计算误差为所有惩罚值之和。

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000235.780.jpg)

初期的误差很大，这是因为来自两个错误点的误差较大。然后我们移动直线，移动过程中会发现有些惩罚值变大，有些惩罚只变小，但总体还是变小的。最终由于所有的错误点都已经被正常归类了，所以误差之和达到最小值。那如何使我的误差值最小呢？这就是我们要看的梯度下降算法（Gradient Descent）

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000307.762.jpg)

## 梯度下降算法
我们把误差想象成一座高峰，当前处于很高的海拔，对应较大的误差值。可以看出海拔高度等于红绿误差面积之和。

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000334.061.jpg)

所以我们向四周探索，寻找最陡峭的下山方向。这相当于求解沿什么方向移动直线才能最大程度减小误差。
我们决定沿这个方向前进一步。这样就只剩一个错误点了。可以看出下山的过程同时见笑了误差函数值。

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000338.932.jpg)

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000348.599.jpg)

然后我们再重复这一过程，沿着最大程度减小误差的方向再前进一步。

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000358.525.jpg)

终于抵达了山底，因为误差已经降低至潜在的最小值

![image](https://raw.githubusercontent.com/vvchenvv/Self_Driving_Tutorial/master/Class1/12_Logistic_regression/7.%20Logistic%20Regression%20Answer.mp4_000402.638.jpg)

以上便是逻辑回归问题的内容




